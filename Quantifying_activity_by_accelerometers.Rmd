---
title: "Practical Machine Learning _ Assignment"
author: "Qian Wang"
date: "January 4, 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

For more details, please visit: http://groupware.les.inf.puc-rio.br/har

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

```{r message=FALSE, warning=FALSE}
library(rpart)
library(knitr)
library(caret)
library(corrplot)
library(dplyr)
library(rpart.plot)
library(randomForest)
library(rattle)
set.seed(2017-01-06)
```


Download the files from the websites supplied:

```{r echo=FALSE, message=FALSE,cache=TRUE}
urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = urltrain, destfile= "/Users/QianWang/Documents/Coursera_8_practicalMachineLearning/week4/training.csv",method = "curl")
download.file(url=urltest, destfile= "/Users/QianWang/Documents/Coursera_8_practicalMachineLearning/week4/test.csv")
```

Read and clean the datas:

```{r}
# training is for the real train and test sets.
training <- read.table(file = "/Users/QianWang/Documents/Coursera_8_practicalMachineLearning/week4/training.csv", header = T, sep = ",")
# test is for the 20 quiz questions.
test <- read.table(file = "/Users/QianWang/Documents/Coursera_8_practicalMachineLearning/week4/test.csv", header = T, sep = ",")

# Some of the variables are flat without much variance, which is uselessful for correlation detection. So we are going to remove them.
flat <- nearZeroVar(training)
training <- training[,-flat]

# There are many variables containing NA, which is annoying for later modelling. We are going to remove the variables that are mostly NAs.
good <- sapply(training,function(x) mean(is.na(x))) <=0.95
training <- training[,good]

# some of the variables are labeles or identification of the observations. They have nothing to do with the correlation or modelling. We will pick them off the data set.
training <- training[,-c(1:5)]

# Now, the data is clean. We want to randomly split the data to trainSet and testSet.
inTrain<- createDataPartition(training$classe,p=0.7,list = F)
trainSet <- training[inTrain,]
testSet <- training[-inTrain,]
```

Explore the data (roughly check out the correlation among the variables, make sure there are no highly correlated variables to slow down the modelling algorithms):

```{r CorrelationMatrix_among_var, fig.width=18, fig.height=18}
# The correlation numbers will be shown in a more eligible way by recruiting corrplot function. The darker red and blue indicates the higher correlation.
corGraph <- cor(trainSet[, colnames(trainSet)!="classe"])
corrplot(corGraph, order = "FPC", method = "number", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0),number.cex = 0.7, number.digits = 2)
```

There are several variables that are highly correlated (>0.90 or < -0.90). In order to save the calculating source, we are going to remove some of the variables.

```{r highCorVarRemove}
# The redundant variables have been detected according to the correlation number shown above. Those variables which share over 0.90 (or less than -0.90) correlation will be removed and left only one variable.
trainSet <- trainSet %>%
        select(-c(accel_belt_y,roll_belt,accel_belt_z,gyros_arm_y,pitch_belt,gyros_dumbbell_z,gyros_dumbbell_x,gyros_forearm_z))
testSet <- testSet %>%
        select(-c(accel_belt_y,roll_belt,accel_belt_z,gyros_arm_y,pitch_belt,gyros_dumbbell_z,gyros_dumbbell_x,gyros_forearm_z))
```


## Modelling and prediction:
In order to obtain the best modelling, we are going to try three different methods (Generalized Boosted Model, Random Forest, Decison Tree) and pick up the one with most accurate prediction rate by predicting and comparing with the testSet "classe", using the three modelling respectively.

### This is first modelling with methods of "gbm":

```{r gbm, message=FALSE, warning=FALSE, cache=TRUE}
# This is the 1st model by using the gbm methods.
set.seed(2017-01-04)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
mod_gbm <- train(classe ~ ., data=trainSet, method = "gbm",
                    trControl = controlGBM, verbose = FALSE)
mod_gbm$finalModel
```

```{r gbm_predict,message=FALSE}
# Predict the results by using the testSet. And calculate the accuracy by comparing with the testSet "classe".
pred_gbm <- predict(mod_gbm, newdata=testSet)
conf_gbm <- confusionMatrix(pred_gbm, testSet$classe)
conf_gbm
```

```{r gbm_plot}
# Plot the prediction restuls.
plot(conf_gbm$table, col = conf_gbm$byClass, 
     main = paste("Accuracy of gbm is ", round(conf_gbm$overall['Accuracy'], 4)))
```

### This is the second modelling with the methods of "rf":

```{r rf, cache=TRUE}
# This is the modelling with the methods of random forest:
set.seed(2017-01-04)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
mod_rf <- train(classe ~ ., data=trainSet, method="rf",
                          trControl=controlRF)
mod_rf$finalModel
```

```{r rf_predict}
# Predict the results with the testSet and compare with the testSet "classe"
pred_rf <- predict(mod_rf, newdata=testSet)
conf_rf <- confusionMatrix(pred_rf, testSet$classe)
conf_rf
```

```{r rf_plot}
# plot the predict result
plot(conf_rf$table, col = conf_rf$byClass, 
     main = paste("Accuracy of the random forest is ",
                  round(conf_rf$overall['Accuracy'], 4)))
```

### This is the third modelling with the methods of "Decision Tree":

```{r dtr, fig.width=18, fig.height=18,cache=TRUE}
# Model with the methods of "decision tree"
set.seed(2017-01-04)
mod_dtr <- rpart(classe ~ ., data=trainSet, method="class")
fancyRpartPlot(mod_dtr)
```

```{r dtr_predict}
# predict the results with the data testSet
pred_dtr <- predict(mod_dtr, newdata=testSet, type="class")
conf_dtr <- confusionMatrix(pred_dtr, testSet$classe)
conf_dtr
```

```{r dtr_plot}
# plot the prediction results
plot(conf_dtr$table, col = conf_dtr$byClass, 
     main = paste("Accuracy of the decison tree is ",
                  round(conf_dtr$overall['Accuracy'], 4)))
```

# Conclusion:
According to the modelling of each methods, the accuracies are:
Generalized boosted Models : 0.9886
Random Forest: 0.9959 
Decision Tree: 0.6984
So, the Random Forest model is most accurate. The Generalized boosted model is second best one. Decision Tree is worst and not reliable.

# For the 20 quiz test questions:
The object "test" contains the data which is used for the practice:

```{r}
quiz <- predict(mod_rf, newdata=test)
quiz
```



